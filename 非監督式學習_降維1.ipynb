{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 降維(Dimension Reduction)\n",
    "顧名思義，就是原本的Data楚瑜在一個比較高的維度座標上，我們希望找到一個低維度的座標來描述它，但又不能失去Data本身的特質"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 為甚麼要降維?\n",
    "1. 壓縮資料，減少計算資源\n",
    "2. 降維可以幫助資料視覺化"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis(PCA)\n",
    "1. 將一個具有n個特徵空間的樣本，轉換維具有k個特徵空間的樣本，其中k<n\n",
    "2. PCA的目的是把高維的點投影到低維的空間上，並且低維度的空間保有高維空間中大部分的性質\n",
    "3. PCA只允許做線性的轉換"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA的主要步驟\n",
    "1. 先求出所有資料中心點mu ，也就是將每一個資料點做平均 \n",
    "2. 將每一個資料點減去mu ，這步驟是將資料點平移，平移後原點是所有點的中心\n",
    "3. 計算每一個feature 的variance\n",
    "4. 把每一個值都除以variance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA 小結\n",
    "1. PCA是個相當直觀且有效的降維方式，不過在三維轉換成二維時我們可以看到，有些數據的集群完全被搗成一團。這跟PCA的原理有關，因為PCA是對資料求共變異數矩陣，在進行奇異值分解。因此會被資料的差異性影響，無法很好表現相似性以及分布\n",
    "2. 且PCA是一種線性降維方式，但如果特徵與特徵間的關聯是非線性關係的話，用PCA可能會導致欠擬合(underfitting)的情形發生"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "1. 目標跟PCA一樣，希望把高維的資料投影到低維中，並且保留高維中的點與點之間的關係與特性。兩者不同的點在於 t-SNE允許非線性的轉換 \n",
    "2. t-SNE使用了更複雜的公式來表達高維與低維之間的關係。主要是將高維的數據用高斯分布的機率密度函數近似，而低維數據的部分使用t分布的方式來近似\n",
    "\n",
    "![image.png](./img/t-SNEformula.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 兩個分布之間的相似度\n",
    "求算兩個分布之間的相似度，經常用KL距離(Kullback-Leibler Divergence)來表示，也叫做相對商(Relative Entropy)\n",
    "\n",
    "![image.png](./img/t-SNEformula2.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE 小結\n",
    "1. 利用此方法降維後原本相近的點依然相近，反之原本距離遠的投影後依然保持遠的距離\n",
    "2. t-SNE允許非線性的轉換，因此有機會在原本分開的三群在做完投影後依然是開的\n",
    "\n",
    "#### t-SNE 不是用於新資料\n",
    "PCA降維可以適用新資料，可呼叫transform()函示即可。而t-SNE則不行。因為演算法的關係在scikit-learn 套件中的t-SNE演算法並沒有transform()函示可以呼叫"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA&t-SNE整理\n",
    "PCA和t-SNE是兩個不同降維的方法\n",
    "PCA的優點在於簡單若新的點要映射時直接帶入公式即可得出降維後的點。若t-SNE有新的點進來時我們沒有去計算新的點和舊的點之間的關係因此我們無法將新的點投影下去\n",
    "t-SNE的優點是可以保留原本高維距離較遠的點將維後依然保持遠的距離，因此這些群降維後依然保持群的特性"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b629c3126b5df0b3c19ac5f524890cb3a3a2e86c1a2f2c4b1c29287aa73e65d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
